<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Cubist Regresion Models • Cubist</title>
<!-- jquery --><script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://maxcdn.bootstrapcdn.com/bootswatch/3.3.7/united/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><!-- Font Awesome icons --><link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">
<!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../jquery.sticky-kit.min.js"></script><script src="../pkgdown.js"></script><link href="../extra.css" rel="stylesheet">
<!-- mathjax --><script src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-vignette">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="../index.html">Cubist</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../articles/cubist.html">Introduction</a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right"></ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9">
    <div class="page-header toc-ignore">
      <h1>Cubist Regresion Models</h1>
            
          </div>

    
    
<div class="contents">
<p><code>Cubist</code> is an <code>R</code> port of the Cubist GPL <code>C</code> code released by RuleQuest at <a href="http://rulequest.com/cubist-info.html"><code>http://rulequest.com/cubist-info.html</code></a>. See the last section of this document for information on the porting. The other parts describes the functionality of the <code>R</code> package.</p>
<div id="model-trees" class="section level2">
<h2 class="hasAnchor">
<a href="#model-trees" class="anchor"></a>Model Trees</h2>
<p>Cubist is a rule–based model that is an extension of Quinlan’s M5 model tree. A tree is grown where the terminal leaves contain linear regression models. These models are based on the predictors used in previous splits. Also, there are intermediate linear models at each step of the tree. A prediction is made using the linear regression model at the terminal node of the tree, but is “smoothed” by taking into account the prediction from the linear model in the previous node of the tree (which also occurs recursively up the tree). The tree is reduced to a set of rules, which initially are paths from the top of the tree to the bottom. Rules are eliminated via pruning and/or combined for simplification.</p>
<p>This is explained better in Quinlan (1992). Wang and Witten (1997) attempted to recreate this model using a “rational reconstruction” of Quinlan (1992) that is the basis for the <code>M5P</code> model in <code>Weka</code> (and the R package <code>RWeka</code>).</p>
<p>An example of a model tree can be illustrated using the Boston Housing data in the <code>mlbench</code> package.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(Cubist)
<span class="kw">library</span>(mlbench)

<span class="kw">data</span>(BostonHousing)
BostonHousing<span class="op">$</span>chas &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(BostonHousing<span class="op">$</span>chas) <span class="op">-</span><span class="st"> </span><span class="dv">1</span>

<span class="kw">set.seed</span>(<span class="dv">1</span>)
inTrain &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(BostonHousing), <span class="kw">floor</span>(.<span class="dv">8</span><span class="op">*</span><span class="kw">nrow</span>(BostonHousing)))

train_pred &lt;-<span class="st"> </span>BostonHousing[ inTrain, <span class="op">-</span><span class="dv">14</span>]
test_pred  &lt;-<span class="st"> </span>BostonHousing[<span class="op">-</span>inTrain, <span class="op">-</span><span class="dv">14</span>]

train_resp &lt;-<span class="st"> </span>BostonHousing<span class="op">$</span>medv[ inTrain]
test_resp  &lt;-<span class="st"> </span>BostonHousing<span class="op">$</span>medv[<span class="op">-</span>inTrain]

model_tree &lt;-<span class="st"> </span><span class="kw"><a href="../reference/cubist.default.html">cubist</a></span>(<span class="dt">x =</span> train_pred, <span class="dt">y =</span> train_resp)
model_tree</code></pre></div>
<pre><code>## 
## Call:
## cubist.default(x = train_pred, y = train_resp)
## 
## Number of samples: 404 
## Number of predictors: 13 
## 
## Number of committees: 1 
## Number of rules: 4</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(model_tree)</code></pre></div>
<pre><code>## 
## Call:
## cubist.default(x = train_pred, y = train_resp)
## 
## 
## Cubist [Release 2.07 GPL Edition]  Fri Nov 24 12:42:53 2017
## ---------------------------------
## 
##     Target attribute `outcome'
## 
## Read 404 cases (14 attributes) from undefined.data
## 
## Model:
## 
##   Rule 1: [88 cases, mean 13.81, range 5 to 27.5, est err 2.10]
## 
##     if
##  nox &gt; 0.668
##     then
##  outcome = 2.07 + 3.14 dis - 0.35 lstat + 18.8 nox + 0.007 b
##            - 0.12 ptratio - 0.008 age - 0.02 crim
## 
##   Rule 2: [153 cases, mean 19.54, range 8.1 to 31, est err 2.16]
## 
##     if
##  nox &lt;= 0.668
##  lstat &gt; 9.59
##     then
##  outcome = 34.81 - 1 dis - 0.72 ptratio - 0.056 age - 0.19 lstat + 1.5 rm
##            - 0.11 indus + 0.004 b
## 
##   Rule 3: [39 cases, mean 24.10, range 11.9 to 50, est err 2.73]
## 
##     if
##  rm &lt;= 6.23
##  lstat &lt;= 9.59
##     then
##  outcome = 11.89 + 3.69 crim - 1.25 lstat + 3.9 rm - 0.0045 tax
##            - 0.16 ptratio
## 
##   Rule 4: [128 cases, mean 31.31, range 16.5 to 50, est err 2.95]
## 
##     if
##  rm &gt; 6.23
##  lstat &lt;= 9.59
##     then
##  outcome = -1.13 + 1.6 crim - 0.93 lstat + 8.6 rm - 0.0141 tax
##            - 0.83 ptratio - 0.47 dis - 0.019 age - 1.1 nox
## 
## 
## Evaluation on training data (404 cases):
## 
##     Average  |error|               2.27
##     Relative |error|               0.34
##     Correlation coefficient        0.94
## 
## 
##  Attribute usage:
##    Conds  Model
## 
##     78%   100%    lstat
##     59%    53%    nox
##     41%    78%    rm
##           100%    ptratio
##            90%    age
##            90%    dis
##            62%    crim
##            59%    b
##            41%    tax
##            38%    indus
## 
## 
## Time: 0.0 secs</code></pre>
<p>There is no formula method for <code>cubist</code>; the predictors are specified as matrix or data frame and the outcome is a numeric vector.</p>
<p>There is a predict method for the model:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">model_tree_pred &lt;-<span class="st"> </span><span class="kw">predict</span>(model_tree, test_pred)
## Test set RMSE
<span class="kw">sqrt</span>(<span class="kw">mean</span>((model_tree_pred <span class="op">-</span><span class="st"> </span>test_resp)<span class="op">^</span><span class="dv">2</span>))</code></pre></div>
<pre><code>## [1] 3.34</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Test set R^2
<span class="kw">cor</span>(model_tree_pred, test_resp)<span class="op">^</span><span class="dv">2</span></code></pre></div>
<pre><code>## [1] 0.857</code></pre>
</div>
<div id="ensembles-by-committees" class="section level2">
<h2 class="hasAnchor">
<a href="#ensembles-by-committees" class="anchor"></a>Ensembles By Committees</h2>
<p>The Cubist model can also use a boosting–like scheme called <em>committees</em> where iterative model trees are created in sequence. The first tree follows the procedure described in the last section. Subsequent trees are created using adjusted versions to the training set outcome: if the model over–predicted a value, the response is adjusted downward for the next model (and so on). Unlike traditional boosting, stage weights for each committee are not used to average the predictions from each model tree; the final prediction is a simple average of the predictions from each model tree.</p>
<p>The <code>committee</code> option can be used to control number of model trees:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">1</span>)
com_model &lt;-<span class="st"> </span><span class="kw"><a href="../reference/cubist.default.html">cubist</a></span>(<span class="dt">x =</span> train_pred, <span class="dt">y =</span> train_resp, <span class="dt">committees =</span> <span class="dv">5</span>)
<span class="kw">summary</span>(com_model)</code></pre></div>
<pre><code>## 
## Call:
## cubist.default(x = train_pred, y = train_resp, committees = 5)
## 
## 
## Cubist [Release 2.07 GPL Edition]  Fri Nov 24 12:42:53 2017
## ---------------------------------
## 
##     Target attribute `outcome'
## 
## Read 404 cases (14 attributes) from undefined.data
## 
## Model 1:
## 
##   Rule 1/1: [88 cases, mean 13.81, range 5 to 27.5, est err 2.10]
## 
##     if
##  nox &gt; 0.668
##     then
##  outcome = 2.07 + 3.14 dis - 0.35 lstat + 18.8 nox + 0.007 b
##            - 0.12 ptratio - 0.008 age - 0.02 crim
## 
##   Rule 1/2: [153 cases, mean 19.54, range 8.1 to 31, est err 2.16]
## 
##     if
##  nox &lt;= 0.668
##  lstat &gt; 9.59
##     then
##  outcome = 34.81 - 1 dis - 0.72 ptratio - 0.056 age - 0.19 lstat + 1.5 rm
##            - 0.11 indus + 0.004 b
## 
##   Rule 1/3: [39 cases, mean 24.10, range 11.9 to 50, est err 2.73]
## 
##     if
##  rm &lt;= 6.23
##  lstat &lt;= 9.59
##     then
##  outcome = 11.89 + 3.69 crim - 1.25 lstat + 3.9 rm - 0.0045 tax
##            - 0.16 ptratio
## 
##   Rule 1/4: [128 cases, mean 31.31, range 16.5 to 50, est err 2.95]
## 
##     if
##  rm &gt; 6.23
##  lstat &lt;= 9.59
##     then
##  outcome = -1.13 + 1.6 crim - 0.93 lstat + 8.6 rm - 0.0141 tax
##            - 0.83 ptratio - 0.47 dis - 0.019 age - 1.1 nox
## 
## Model 2:
## 
##   Rule 2/1: [71 cases, mean 13.41, range 5 to 27.5, est err 2.66]
## 
##     if
##  crim &gt; 5.69175
##  dis &gt; 1.4254
##     then
##  outcome = 42.13 + 2.45 dis - 0.47 lstat - 0.71 ptratio - 1.8 rm
## 
##   Rule 2/2: [84 cases, mean 18.75, range 8.1 to 27.5, est err 2.25]
## 
##     if
##  crim &lt;= 5.69175
##  nox &gt; 0.532
##  dis &gt; 1.4254
##  tax &gt; 222
##  ptratio &gt; 17
##     then
##  outcome = 44.08 + 1.19 crim - 0.43 lstat - 1.05 ptratio - 0.011 age
## 
##   Rule 2/3: [15 cases, mean 23.43, range 5 to 50, est err 5.62]
## 
##     if
##  dis &lt;= 1.4254
##  ptratio &gt; 17
##     then
##  outcome = 174.86 - 100.95 dis - 1.07 lstat - 0.09 ptratio
## 
##   Rule 2/4: [77 cases, mean 23.90, range 11.8 to 50, est err 2.37]
## 
##     if
##  ptratio &lt;= 17
##  lstat &gt; 5.12
##     then
##  outcome = -3.3 + 8.3 rm - 0.0238 tax - 1.66 dis - 0.063 age - 0.1 lstat
##            - 0.21 ptratio - 3.8 nox + 0.007 zn
## 
##   Rule 2/5: [128 cases, mean 25.56, range 14.4 to 50, est err 3.12]
## 
##     if
##  crim &lt;= 5.69175
##  nox &lt;= 0.532
##  ptratio &gt; 17
##     then
##  outcome = -15.58 + 2.43 crim + 7.1 rm - 0.075 age + 0.24 lstat
##            - 0.41 dis - 0.16 ptratio
## 
##   Rule 2/6: [16 cases, mean 27.91, range 15.7 to 39.8, est err 5.25]
## 
##     if
##  tax &lt;= 222
##  lstat &gt; 5.12
##     then
##  outcome = 274.62 - 12.31 ptratio - 0.212 age - 0.03 lstat
## 
##   Rule 2/7: [18 cases, mean 30.49, range 22.5 to 50, est err 3.69]
## 
##     if
##  rm &lt;= 6.861
##  lstat &lt;= 5.12
##     then
##  outcome = -58.03 + 10.96 crim + 13.3 rm - 0.03 lstat - 0.08 dis
##            - 0.06 ptratio - 1.1 nox
## 
##   Rule 2/8: [19 cases, mean 41.54, range 31.2 to 50, est err 3.63]
## 
##     if
##  rm &gt; 6.861
##  age &lt;= 71
##  lstat &lt;= 5.12
##     then
##  outcome = -56.93 + 14.2 rm - 0.07 lstat - 0.2 dis - 2.6 nox
##            - 0.13 ptratio + 0.006 zn
## 
##   Rule 2/9: [14 cases, mean 43.48, range 22.8 to 50, est err 5.55]
## 
##     if
##  age &gt; 71
##  lstat &lt;= 5.12
##     then
##  outcome = -24.48 + 1.99 crim + 0.467 age + 3.5 rm
## 
## Model 3:
## 
##   Rule 3/1: [88 cases, mean 13.81, range 5 to 27.5, est err 2.32]
## 
##     if
##  nox &gt; 0.668
##     then
##  outcome = -9 + 5.5 dis + 19.4 nox + 0.014 b - 0.12 lstat - 0.16 ptratio
##            - 0.04 crim
## 
##   Rule 3/2: [10 cases, mean 17.64, range 11.7 to 27.5, est err 11.68]
## 
##     if
##  nox &lt;= 0.668
##  b &lt;= 179.36
##     then
##  outcome = -2.07 + 0.149 b + 0.77 lstat
## 
##   Rule 3/3: [156 cases, mean 19.68, range 8.1 to 33.8, est err 2.23]
## 
##     if
##  nox &lt;= 0.668
##  lstat &gt; 9.53
##     then
##  outcome = 28.56 - 1.09 dis - 0.27 lstat - 0.068 age + 2.6 rm
##            - 0.6 ptratio
## 
##   Rule 3/4: [164 cases, mean 29.68, range 11.9 to 50, est err 3.44]
## 
##     if
##  lstat &lt;= 9.53
##     then
##  outcome = 6.57 + 4.08 crim - 0.75 lstat + 7.6 rm - 0.0301 tax
##            - 0.79 ptratio - 0.15 dis - 2.2 nox + 0.001 b
## 
## Model 4:
## 
##   Rule 4/1: [335 cases, mean 19.44, range 5 to 50, est err 2.69]
## 
##     if
##  rm &lt;= 7.079
##  lstat &gt; 5.12
##     then
##  outcome = 45.08 - 0.4 lstat + 0.27 rad - 0.0124 tax - 0.2 crim
##            - 0.6 ptratio - 8.5 nox - 0.36 dis - 0.04 indus
## 
##   Rule 4/2: [19 cases, mean 20.96, range 5 to 50, est err 6.81]
## 
##     if
##  rm &lt;= 7.079
##  dis &lt;= 1.4261
##     then
##  outcome = 163.2 - 85.4 dis - 1.21 lstat - 0.15 crim
## 
##   Rule 4/3: [111 cases, mean 23.01, range 14.4 to 32, est err 1.92]
## 
##     if
##  nox &lt;= 0.51
##  rm &lt;= 7.079
##  tax &gt; 193
##  lstat &gt; 5.12
##     then
##  outcome = 9.18 + 12.12 crim + 2.8 rm - 0.031 age - 0.05 lstat + 0.04 rad
##            - 0.002 tax - 0.1 ptratio - 0.1 dis - 1.6 nox
## 
##   Rule 4/4: [9 cases, mean 24.33, range 15.7 to 36.2, est err 7.38]
## 
##     if
##  rm &lt;= 7.079
##  tax &lt;= 193
##  lstat &gt; 5.12
##     then
##  outcome = 22.72
## 
##   Rule 4/5: [18 cases, mean 30.49, range 22.5 to 50, est err 4.91]
## 
##     if
##  rm &lt;= 6.861
##  lstat &lt;= 5.12
##     then
##  outcome = 20.95 + 8.16 crim - 0.54 lstat + 0.23 rad + 1.3 rm
## 
##   Rule 4/6: [35 cases, mean 36.15, range 22.5 to 50, est err 3.61]
## 
##     if
##  age &lt;= 71
##  lstat &lt;= 5.12
##     then
##  outcome = -67.4 + 15.9 rm - 1.05 rad - 0.005 b - 0.05 lstat
## 
##   Rule 4/7: [43 cases, mean 39.37, range 15 to 50, est err 6.37]
## 
##     if
##  rm &gt; 7.079
##     then
##  outcome = -123.73 + 0.308 b + 8.8 rm - 0.45 rad - 1.38 ptratio
##            - 0.04 lstat - 0.0016 tax - 0.1 dis - 1.2 nox - 0.02 indus
##            - 0.01 crim
## 
##   Rule 4/8: [14 cases, mean 43.48, range 22.8 to 50, est err 5.14]
## 
##     if
##  age &gt; 71
##  lstat &lt;= 5.12
##     then
##  outcome = -34.28 + 0.598 age - 0.75 lstat + 6.1 rm - 0.047 b + 0.16 rad
## 
## Model 5:
## 
##   Rule 5/1: [88 cases, mean 13.81, range 5 to 27.5, est err 2.73]
## 
##     if
##  nox &gt; 0.668
##     then
##  outcome = -35.12 + 8.59 dis + 38.7 nox + 0.017 b - 0.04 lstat
##            - 0.07 ptratio + 0.01 rad + 0.1 rm
## 
##   Rule 5/2: [156 cases, mean 19.68, range 8.1 to 33.8, est err 2.53]
## 
##     if
##  nox &lt;= 0.668
##  lstat &gt; 9.53
##     then
##  outcome = 44.88 - 1.48 dis - 0.076 age - 0.28 lstat - 0.8 ptratio
##            + 0.012 b + 0.1 rad + 0.3 rm - 1.6 nox - 0.0007 tax
## 
##   Rule 5/3: [189 cases, mean 24.76, range 12.7 to 50, est err 2.41]
## 
##     if
##  dis &gt; 3.3175
##     then
##  outcome = -24.62 + 1.13 crim + 10.4 rm - 0.0183 tax - 0.69 dis
##            - 0.19 lstat - 0.043 age - 0.26 ptratio + 0.022 zn
## 
##   Rule 5/4: [44 cases, mean 35.04, range 11.9 to 50, est err 6.37]
## 
##     if
##  dis &lt;= 3.3175
##  lstat &lt;= 9.53
##     then
##  outcome = 32.74 + 6.34 crim - 0.0468 tax - 0.87 lstat + 5.5 rm
##            - 1.16 ptratio
## 
## 
## Evaluation on training data (404 cases):
## 
##     Average  |error|               1.91
##     Relative |error|               0.29
##     Correlation coefficient        0.96
## 
## 
##  Attribute usage:
##    Conds  Model
## 
##     65%    99%    lstat
##     46%    56%    nox
##     32%    71%    rm
##     18%    88%    dis
##     13%    95%    ptratio
##     12%    65%    crim
##      9%    55%    tax
##      4%    56%    age
##            36%    b
##            34%    rad
##            23%    indus
##            12%    zn
## 
## 
## Time: 0.0 secs</code></pre>
<p>For this model:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">com_pred &lt;-<span class="st"> </span><span class="kw">predict</span>(com_model, test_pred)
## RMSE
<span class="kw">sqrt</span>(<span class="kw">mean</span>((com_pred <span class="op">-</span><span class="st"> </span>test_resp)<span class="op">^</span><span class="dv">2</span>))</code></pre></div>
<pre><code>## [1] 2.87</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## R^2
<span class="kw">cor</span>(com_pred, test_resp)<span class="op">^</span><span class="dv">2</span></code></pre></div>
<pre><code>## [1] 0.896</code></pre>
</div>
<div id="instancebased-corrections" class="section level2">
<h2 class="hasAnchor">
<a href="#instancebased-corrections" class="anchor"></a>Instance–Based Corrections</h2>
<p>Another innovation in Cubist using nearest–neighbors to adjust the predictions from the rule–based model. First, a model tree (with or without committees) is created. Once a sample is predicted by this model, Cubist can find it’s nearest neighbors and determine the average of these training set points. See Quinlan (1993a) for the details of the adjustment.</p>
<p>The development of rules and committees is independent of the choice of using instances. The original <code>C</code> code allowed the program to choose whether to use instances, not use them or let the program decide. Our approach is to build a model with the <code>cubist</code> function that is ignorant to the decision about instances. When samples are predicted, the argument <code>neighbors</code> can be used to adjust the rule–based model predictions (or not).</p>
<p>We can add instances to the previously fit committee model:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">inst_pred &lt;-<span class="st"> </span><span class="kw">predict</span>(com_model, test_pred, <span class="dt">neighbors =</span> <span class="dv">5</span>)
## RMSE
<span class="kw">sqrt</span>(<span class="kw">mean</span>((inst_pred <span class="op">-</span><span class="st"> </span>test_resp)<span class="op">^</span><span class="dv">2</span>))</code></pre></div>
<pre><code>## [1] 2.69</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## R^2
<span class="kw">cor</span>(inst_pred, test_resp)<span class="op">^</span><span class="dv">2</span></code></pre></div>
<pre><code>## [1] 0.911</code></pre>
<p>Note that the previous models used the implicit default of <code>neighbors = 0</code> for their predictions.</p>
<p>To tune the model over different values of <code>neighbors</code> and <code>committees</code>, the <code>train</code> function in the <a href="hhttps://CRAN.R-project.org/package=caret">`caret</a> package can be used to optimize these parameters. For example:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(caret)

grid &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(<span class="dt">committees =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">10</span>, <span class="dv">50</span>, <span class="dv">100</span>),
                    <span class="dt">neighbors =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">5</span>, <span class="dv">9</span>))
<span class="kw">set.seed</span>(<span class="dv">1</span>)
boston_tuned &lt;-<span class="st"> </span><span class="kw"><a href="http://www.rdocumentation.org/packages/caret/topics/train">train</a></span>(
  <span class="dt">x =</span> train_pred,
  <span class="dt">y =</span> train_resp,
  <span class="dt">method =</span> <span class="st">"cubist"</span>,
  <span class="dt">tuneGrid =</span> grid,
  <span class="dt">trControl =</span> <span class="kw"><a href="http://www.rdocumentation.org/packages/caret/topics/trainControl">trainControl</a></span>(<span class="dt">method =</span> <span class="st">"cv"</span>)
  )
boston_tuned</code></pre></div>
<pre><code>## Cubist 
## 
## 404 samples
##  13 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 363, 365, 364, 363, 364, 364, ... 
## Resampling results across tuning parameters:
## 
##   committees  neighbors  RMSE  Rsquared  MAE 
##     1         0          4.44  0.774     2.80
##     1         1          4.24  0.795     2.81
##     1         5          4.06  0.810     2.57
##     1         9          4.11  0.805     2.56
##    10         0          3.68  0.839     2.45
##    10         1          3.61  0.844     2.39
##    10         5          3.40  0.859     2.22
##    10         9          3.44  0.856     2.23
##    50         0          3.46  0.856     2.35
##    50         1          3.41  0.860     2.28
##    50         5          3.22  0.874     2.12
##    50         9          3.26  0.871     2.14
##   100         0          3.42  0.859     2.33
##   100         1          3.39  0.862     2.26
##   100         5          3.18  0.877     2.11
##   100         9          3.22  0.873     2.13
## 
## RMSE was used to select the optimal model using the smallest value.
## The final values used for the model were committees = 100 and neighbors
##  = 5.</code></pre>
<p>The next figure shows the profiles of the tuning parameters produced using <code>ggplot(boston_tuned)</code>.</p>
<pre><code>## Warning: Ignoring unknown aesthetics: shape</code></pre>
<p><img src="cubist_files/figure-html/plot-tune-1.png" width="672"></p>
<p>It may also be useful to see how the different models fit a single predictor:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lstat_df &lt;-<span class="st"> </span>train_pred[, <span class="st">"lstat"</span>, drop =<span class="st"> </span><span class="ot">FALSE</span>]
rules_only &lt;-<span class="st"> </span><span class="kw"><a href="../reference/cubist.default.html">cubist</a></span>(<span class="dt">x =</span> lstat_df, <span class="dt">y =</span> train_resp)
rules_and_com &lt;-<span class="st"> </span><span class="kw"><a href="../reference/cubist.default.html">cubist</a></span>(<span class="dt">x =</span> lstat_df, <span class="dt">y =</span> train_resp, <span class="dt">committees =</span> <span class="dv">100</span>)

predictions &lt;-<span class="st"> </span>lstat_df
predictions<span class="op">$</span>medv &lt;-<span class="st"> </span>train_resp
predictions<span class="op">$</span>rules_neigh &lt;-<span class="st"> </span><span class="kw">predict</span>(rules_only, lstat_df, <span class="dt">neighbors =</span> <span class="dv">5</span>)
predictions<span class="op">$</span>committees &lt;-<span class="st"> </span><span class="kw">predict</span>(rules_and_com, lstat_df)</code></pre></div>
<p>The figure below shows the model fits for the test data. For these data, there doesn’t appear to be much of a improvement when committees or instances are added to the based rules.</p>
<p><img src="cubist_files/figure-html/lstatPlot-1.png" width="672"></p>
</div>
<div id="variable-importance" class="section level2">
<h2 class="hasAnchor">
<a href="#variable-importance" class="anchor"></a>Variable Importance</h2>
<p>The <code>modelTree</code> method for Cubist shows the usage of each variable in either the rule conditions or the (terminal) linear model. In actuality, many more linear models are used in prediction that are shown in the output. Because of this, the variable usage statistics shown at the end of the output of the <code>summary</code> function will probably be inconsistent with the rules also shown in the output. At each split of the tree, Cubist saves a linear model (after feature selection) that is allowed to have terms for each variable used in the current split or any split above it. Quinlan (1992) discusses a smoothing algorithm where each model prediction is a linear combination of the parent and child model along the tree. As such, the final prediction is a function of all the linear models from the initial node to the terminal node. The percentages shown in the Cubist output reflects all the models involved in prediction (as opposed to the terminal models shown in the output).</p>
<p>The raw usage statistics are contained in a data frame called <code>usage</code> in the <code>cubist</code> object.</p>
<p>The <code>caret</code> package has a general variable importance method <code>varImp</code>. When using this function on a <code>cubist</code> argument, the variable importance is a linear combination of the usage in the rule conditions and the model.</p>
<p>For example:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(model_tree)</code></pre></div>
<pre><code>## 
## Call:
## cubist.default(x = train_pred, y = train_resp)
## 
## 
## Cubist [Release 2.07 GPL Edition]  Fri Nov 24 12:42:53 2017
## ---------------------------------
## 
##     Target attribute `outcome'
## 
## Read 404 cases (14 attributes) from undefined.data
## 
## Model:
## 
##   Rule 1: [88 cases, mean 13.81, range 5 to 27.5, est err 2.10]
## 
##     if
##  nox &gt; 0.668
##     then
##  outcome = 2.07 + 3.14 dis - 0.35 lstat + 18.8 nox + 0.007 b
##            - 0.12 ptratio - 0.008 age - 0.02 crim
## 
##   Rule 2: [153 cases, mean 19.54, range 8.1 to 31, est err 2.16]
## 
##     if
##  nox &lt;= 0.668
##  lstat &gt; 9.59
##     then
##  outcome = 34.81 - 1 dis - 0.72 ptratio - 0.056 age - 0.19 lstat + 1.5 rm
##            - 0.11 indus + 0.004 b
## 
##   Rule 3: [39 cases, mean 24.10, range 11.9 to 50, est err 2.73]
## 
##     if
##  rm &lt;= 6.23
##  lstat &lt;= 9.59
##     then
##  outcome = 11.89 + 3.69 crim - 1.25 lstat + 3.9 rm - 0.0045 tax
##            - 0.16 ptratio
## 
##   Rule 4: [128 cases, mean 31.31, range 16.5 to 50, est err 2.95]
## 
##     if
##  rm &gt; 6.23
##  lstat &lt;= 9.59
##     then
##  outcome = -1.13 + 1.6 crim - 0.93 lstat + 8.6 rm - 0.0141 tax
##            - 0.83 ptratio - 0.47 dis - 0.019 age - 1.1 nox
## 
## 
## Evaluation on training data (404 cases):
## 
##     Average  |error|               2.27
##     Relative |error|               0.34
##     Correlation coefficient        0.94
## 
## 
##  Attribute usage:
##    Conds  Model
## 
##     78%   100%    lstat
##     59%    53%    nox
##     41%    78%    rm
##           100%    ptratio
##            90%    age
##            90%    dis
##            62%    crim
##            59%    b
##            41%    tax
##            38%    indus
## 
## 
## Time: 0.0 secs</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">model_tree<span class="op">$</span>usage</code></pre></div>
<pre><code>##    Conditions Model Variable
## 1          78   100    lstat
## 2          59    53      nox
## 3          41    78       rm
## 4           0   100  ptratio
## 5           0    90      age
## 6           0    90      dis
## 7           0    62     crim
## 8           0    59        b
## 9           0    41      tax
## 10          0    38    indus
## 11          0     0       zn
## 12          0     0     chas
## 13          0     0      rad</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(caret)
<span class="kw"><a href="http://www.rdocumentation.org/packages/caret/topics/varImp">varImp</a></span>(model_tree)</code></pre></div>
<pre><code>##         Overall
## lstat      89.0
## nox        56.0
## rm         59.5
## ptratio    50.0
## age        45.0
## dis        45.0
## crim       31.0
## b          29.5
## tax        20.5
## indus      19.0
## zn          0.0
## chas        0.0
## rad         0.0</code></pre>
<p>It should be noted that this variable importance measure does not capture the influence of the predictors when using the instance–based correction.</p>
</div>
<div id="exporting-the-model" class="section level2">
<h2 class="hasAnchor">
<a href="#exporting-the-model" class="anchor"></a>Exporting the Model</h2>
<p>As previously mentioned, this code is a port of the command–line <code>C</code> code. To run the <code>C</code> code, the training set data must be converted to a specific file format as detailed on the RuleQuest website. Two files are created. The <code>file.data</code> file is a header–less, comma delimited version of the data (the <code>file</code> part is a name given by the user). The <code>file.names</code> file provides information about the columns (eg. levels for categorical data and so on). After running the <code>C</code> program, another text file called <code>file.models</code>, which contains the information needed for prediction.</p>
<p>Once a model has been built with the <code>R</code> <code>cubist</code> package, the <code>exportCubistFiles</code> can be used to create the <code>.data</code>, <code>.names</code> and <code>.model</code> files so that the same model can be run at the command–line.</p>
</div>
<div id="current-limitations" class="section level2">
<h2 class="hasAnchor">
<a href="#current-limitations" class="anchor"></a>Current Limitations</h2>
<p>There are a few features in the <code>C</code> code that are not yet operational in the <code>R</code> package:</p>
<ul>
<li>only continuous and categorical predictors can be used (the original source code allows for other data types)</li>
<li>there is an option to let the <code>C</code> code decide on using instances or not. The choice is more explicit in this package</li>
<li>non–standard predictor names are not currently checked/fixed</li>
<li>the <code>C</code> code supports binning of predictors</li>
</ul>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
        <div id="tocnav">
      <h2 class="hasAnchor">
<a href="#tocnav" class="anchor"></a>Contents</h2>
      <ul class="nav nav-pills nav-stacked">
<li><a href="#model-trees">Model Trees</a></li>
      <li><a href="#ensembles-by-committees">Ensembles By Committees</a></li>
      <li><a href="#instancebased-corrections">Instance–Based Corrections</a></li>
      <li><a href="#variable-importance">Variable Importance</a></li>
      <li><a href="#exporting-the-model">Exporting the Model</a></li>
      <li><a href="#current-limitations">Current Limitations</a></li>
      </ul>
</div>
      </div>

</div>


      <footer><div class="copyright">
  <p>Developed by Max Kuhn, Quinlan Ross.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://hadley.github.io/pkgdown/">pkgdown</a>.</p>
</div>

      </footer>
</div>

  </body>
</html>
